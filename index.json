[
{
	"uri": "/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "This workshop cover concepts, techiques, and tools, required to perform troubleshoot of problems commonly seen in production Kubernetes cluster at scale, some of the topics covered are:\n Kubernetes Components Kubernetes Logs and Metrics The Kubernetes Scheduler  Priorities QoS Node Selection   Kubernetes Networking  CoreDNS Services Ingresses Amazon VPC CNI for Kubernetes   Inspect Kubernetes traffic in real time Look for performance problems Kubernetes scaling (HPA and Cluster Autoscaler) Look for node level problems  During 3 hours we will be learning about advanced Kubernetes concept but also we will be introduced to real failure use case that we need to troubleshoot in order to fix and recover the cluster. So, by the end of this workshop you should be proeficient on how to troubleshoot Kubernetes problems and know the different components of a Kubernetes cluster that your monitoring systems.\nThe workshop target are DevOps, and SREs that administer EKS/Kubernetes clusters in production. So, to attend to this workshop it is important to have some Network, Linux, AWS, and Kubernetes background.\nThe Infrastructure In this workshop we will have a pre-deployed EKS Cluster in which we will install the following components:\n AWS EBS CSI Driver Cluster Autoscaler AWS Load Balancer Controller Metrics Server Litmus Chaos Engineering tool Weaveworks Sock Shop microservice  During the installation of all of those components we will be checking the status of each addon but also trobleshoot if something is wrong and fix it.\nIn addition we will deploy a single ECS Fargate cluster using a service that will generate traffic against our microservice running in EKS.\n"
},
{
	"uri": "/concepts/",
	"title": "Concepts",
	"tags": [],
	"description": "",
	"content": "In this section we will describe the main Kubernetes and EKS concepts we need to understand previous to start operating a Kubernetes cluster.\n Kubernetes Components EKS Components  "
},
{
	"uri": "/concepts/k8s/",
	"title": "Kubernetes Components",
	"tags": [],
	"description": "",
	"content": "Kubernetes Control Plane and Data Plane components:\n From https://kubernetes.io/docs/concepts/overview/components/\n Control Plane Components  Kubernetes API Server: Kubernetes API front-end, all requests made to the Kubernets cluster pass through this component etcd: Main Kubernetes database, all information of your Kubernetes cluster will be stored in here. Kubernetes Scheduler: Component in charge of placing pods in worker nodes based on a complex set of decision rules. Kubernetes Controller Manager (KCM): This component runs all controllers within the cluster, these are managed through a control loop which keep watching the desired state of a kubernetes resource against it current status. Cloud Controller Manager: This is a component that is in charge of managing Cloud resources, the cloud controller manager is compounded by providers that will manage a custom logic depending of the Cloud platform which is being in use. Ex: AWS.  Data Plane Components  All of these components will run in every single node joined to the cluster\n  Kubelet: Main kubernetes agent which is in charge of being sure that a Pod is running in a node, but also inform about Node status to the Control Plane. Kube Proxy: This is the component in charge of managing the Kubernetes service network. So, different pods in a kubernetes cluster can reach each other in a Load Balanced way. Main implementations are via iptables or ipvs. Container Runtime: Container Engine used to create containers in a worker node, such as: Docker, or containerd.  Core Addons  CoreDNS: This component is in charge of managing internal Kubernetes DNS resolution but also serve as DNS resolver by forwarding non-Kubernetes queries to external DNS servers. Container Networking (CNI): Software component in charge of managing Pod network. It will be in charge of allocating and assigning IP to Pods.  Addons (almost core)  Metrics Server: This component collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through the Metrics API. This component is esential for Pod autoscaling, as HPA needs to get Pod resource metric from the Metrics API endpoint. Cluster Autoscaler (or Karpenter): This component is in charge of scaling Worker Nodes based on the number of Pods to be scheduled in the cluster. For the case of AWS, cluster autoscaler will rely on Auto Scaling Groups to add/remove nodes in a cluster. Karpenter is a new Node Scaling solution that doesn\u0026rsquo;t relies on Auto Scaling Groups but instead provision nodes by directly talking to the EC2 control plane. Container Storage Drivers: Software component that will be in charge of provision storage to Pods. There are a big number of drivers out there. AWS currently provide official CSI drivers for: EBS, EFS, FSx for Lustre. Ingress Controllers: Software component that will be in charge of creating advanced Layer 7 routing policies for Kubernetes Services. In the case of AWS the AWS Load Balancer Controller acts as a Ingress Controller and will provide the functionality via Application Load Balancers that will redirect traffic to your pods depending of a predefined set of routing rules.  "
},
{
	"uri": "/concepts/eks/",
	"title": "EKS Components",
	"tags": [],
	"description": "",
	"content": "What is EKS  EKS provides a managed Kubernetes experience for performant, reliable, and secure Kubernetes. EKS makes Kubernetes operations, administration, and management simple and boring. EKS runs vanilla Kubernetes. EKS is upstream and certified conformant version of Kubernetes. EKS supports 4 versions of Kubernetes, giving customers time to test and roll out upgrades.  EKS Architecture and Components EKS Logical Overview  From Amazon EKS Under the Hood https://www.youtube.com/watch?v=7vxDWDD2YnM\n EKS Control Plane  From Amazon EKS Under the Hood https://www.youtube.com/watch?v=7vxDWDD2YnM\n EKS distribution What makes EKS different from other Kubernetes distributions are a set of unique components that are configured by default and helps to integrate EKS seamless with AWS infrastructure:\n EKS Authentication via IAM  OIDC provider integration   EKS VPC CNI  Multiple configuration options: Prefix allocation, IPv6, Custom Networking   Self Managed, or Managed Worker Groups Karpenter a new approach for managing Worker Nodes EKS Fargate integration  "
},
{
	"uri": "/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "AWS Event Engine allows AWS field teams to run Workshops, GameDays, Bootcamps, Immersion Days, and other events that require hands-on access to AWS accounts. Follow the steps below to gain access to your Event Engine AWS account.\n Open a web browser and navigate to the Event Engine login page: https://dashboard.eventengine.run. You will be presented with the terms and condtions login page. Please read and understand the terms and conditions governing the use of Event Engine accounts.\nEvent staff will provide you with a 12 digit team hash. Use the 12 digit hash to login.\n::alert[You may be prompted with an additional login step. If so, ask the event staff for further instruction.]{type=\u0026ldquo;info\u0026rdquo;}\nAfter logging in, you will be redirected to the Team Dashboard. Set your team name if you like. (We have enough accounts for participants to each have their own account, but all participants are welcome to form teams.)\nClick the AWS Console link to login to the AWS account you’ll use for today’s event.\nClick the ‘Open AWS Console’ button to open the AWS Console.\n::alert[Note the region that the event is using. Only actions in this region are allowed.]{type=\u0026ldquo;warning\u0026rdquo;}\nYou should now be able to access the AWS Console.\n Note: this workshop can be run by complete from the Cloud9 environment deployed. However, we will make use of some Kubernetes GUI tools from our own workstation in order to have a broader view of our cluster setup, for this reason is important to setup the AWS CLI credentials presented at this stage.  ::alert[Notify event staff if you have any difficulty.]{type=\u0026ldquo;info\u0026rdquo;}\n"
},
{
	"uri": "/getting-started/using-cloud9/",
	"title": "Using Cloud 9",
	"tags": [],
	"description": "",
	"content": "Visit Cloud9 console and click Shared with you menu option:\nThen click the Open IDE button the troubleshoot-workshop environment.\nConfigure AWS CLI with the current AWS Region as default: Execute the following commands in the terminal:\n This script configures the AWS CLI to use the current region as the default.\n export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region export S3_BUCKET=$(aws cloudformation describe-stacks --stack-name ClusterStack --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SSMOutputsS3Bucket\u0026#39;].OutputValue\u0026#34; --output text) "
},
{
	"uri": "/troubleshooting/",
	"title": "Options for Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Kubernetes offer a set of options to troubleshoot problems in both Kubernetes components, and applications running within the Kubernetes cluster. In this section we\u0026rsquo;re going to dig into the multiple options available at every single Kubernetes/EKS cluster.\nData Plane:  Container Logs Kubernetes events Get and Describe resources Worker Node Logs Metrics  Control Plane:  Control Plane Metrics Control Plane Logs  Bonus A visual guide for troubleshooting Kubernetes deployments\n"
},
{
	"uri": "/troubleshooting/containerlogs/",
	"title": "Data Plane - Container Logs",
	"tags": [],
	"description": "",
	"content": "A common (and best) practice in Kubernetes is applications outputing logs to the Operative System standard output (STDOUT), or standard error (STDERR). Then Kubernetes will automatically collect data from the STDOUT/STDERR and forward to a file within the Worker Node. In EKS these log files are automatically configured to have fixed size in order to avoid filling the Worker Node hard disk, for log retention is recommended to use a log collection agent and forward to a log storage engine such as, Cloudwatch Logs, or Opensearch.\nIn Kubernetes it is possible to watch container logs in real time by using kubectl logs, as alternative there are most advanced tools to watch container logs in realtime some of these tools will be covered in later sections.\nWatching logs At this stage our cluster is configured only with few addons. Let´s see logs from one of the containers in our recently deployed EKS cluster:\nkubectl logs -n kube-system -l k8s-app=kube-proxy As you can see, the kubectl logs command will show current container logs, this command accept several parameters that allow to control how to see the logs, for example. If you want to keep watching container logs in real time, then you can execute the command as follow:\nkubectl logs -n kube-system -f -l k8s-app=kube-proxy Other useful parameters are:\n -p: To see logs from a previous container, useful when a container has been restarted and looking the reason why. -c \u0026lt;container name\u0026gt;: Useful when a Pod is running multiple containers, this parameter will allow you to select which container to see the logs.  As previously mentioned, Kubernetes is only capable to collect information send to the Operative System STDOUT/STDERR hence only logs sent there will be seen by the kubectl logs command.\nWhat if my logs are sent to a file? If your application is not capable to send logs to the STDOUT/STDERR, then Kubernetes won\u0026rsquo;t be able to see those logs, if those logs are sent to a local file within the container then it is hihgly probable to lose those logs if the container is restarted. A common pattern to workaround this problem is to use a sidecar container to read the local log file and forward its content to STDOUT. More information at: https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent\n"
},
{
	"uri": "/troubleshooting/events/",
	"title": "Data Plane - Kubernetes Events",
	"tags": [],
	"description": "",
	"content": "An event is a Kubernetes resource that report a state change within the cluster. Events have a limited retention time. Events are harldy tied to specific resources, meaning that most of the Kubernetes resources such as, Pod, Deployments, Services, etc. generates events informing that some change or action has been executed.\nKubernetes events are namespace scoped and can be retrived by using the kubectl events command, events for a specific resource can be seen by describing the resource, we will see this in next section.\nTo see all events in a Kubernetes cluster you can use the following command:\nkubectl get events --all-namespaces Normally an event will be retained in Kubernetes database 1 hour, once this time is reached the event will be deleted from the cluster database (etcd).\nEvents are an excellent source for troubleshooting problems in a specific Kubernetes resource.\nKey Kubernetes events that can be looked when troubleshooting:\n Failed Events: events reported when a Kubernetes resource can\u0026rsquo;t be created, or an action can\u0026rsquo;t be executed, such as: FailledToCreateContainer, FailedToPullImage. Eviction Events: these events will report when a pod is moved from a worker node to another due to multiple reasons, for example: When a worker node is going to be terminated, or high pressure in the worker node. Scheduling events: these events are triggered when the Kubernetes schduler is looking to allocate a Pod within the cluster.  Sorting events Events are asynchronous and when retreiving events from API Server, these are going to be returned without an specific order, if you want to sort event you need to explicitly specify to kubectl how to sort them, example:\nkubectl get events --sort-by=\u0026#39;{.metadata.creationTimestamp}\u0026#39; --all-namespaces Filtering events Events can be filtered by field selectors, for example to see all NOT normal events you can execute the following command:\nkubectl get events --all-namespaces --field-selector type!=Normal "
},
{
	"uri": "/troubleshooting/describe/",
	"title": "Data Plane - Get and Describe resources",
	"tags": [],
	"description": "",
	"content": "The kubectl command offer several options for rapid troubleshooting, we already mentioned the logs subcommand. In this section we are going to show the get and describe commands.\nkubectl get This command allow you to query resources installed in the Kubernetes cluster, the basic syntax of this command is: kubectl get [OPTIONS] RESOURCE TYPE [RESOURCE NAME]\nExample:\nkubectl get nodes NAME STATUS ROLES AGE VERSION\rip-10-10-1-155.eu-central-1.compute.internal Ready \u0026lt;none\u0026gt; 27d v1.21.5-eks-9017834\rip-10-10-1-209.eu-central-1.compute.internal Ready \u0026lt;none\u0026gt; 15d v1.21.5-eks-9017834\rip-10-10-1-89.eu-central-1.compute.internal Ready \u0026lt;none\u0026gt; 24d v1.21.5-eks-9017834\rip-10-10-1-98.eu-central-1.compute.internal Ready \u0026lt;none\u0026gt; 26d v1.21.5-eks-9017834\rDepending on the resource being inspected the output may differ a bit, in this case the command is returning a list of Worker Nodes joined to the cluster, and important field in here for debugging purposes is STATUS as it will show the current status of the Worker Node.\nLet\u0026rsquo;s look at another example:\nkubectl get configmaps -n kube-system NAME DATA AGE\raws-auth 3 35d\rcoredns 1 35d\rcp-vpc-resource-controller 0 35d\reks-certificates-controller 0 35d\rextension-apiserver-authentication 6 35d\rkube-proxy 1 35d\rkube-proxy-config 1 35d\rkube-root-ca.crt 1 35d\rHere the command is returning a list of configmaps in namespace \u0026ldquo;kube-system\u0026rdquo; you can see here how the output differs from the one for nodes. It is important to notice that this command is scoped to namespaces. So, if you don\u0026rsquo;t specify the namespace of the resoruce then it will return for the namespace configured in your current kubectl context.\nThere is a special keyword that can be used with the get command to get \u0026ldquo;all\u0026rdquo; resources in a namespace:\nkubectl get all -n kube-system NAME READY STATUS RESTARTS AGE\rpod/aws-node-j8zsc 1/1 Running 0 34d\rpod/aws-node-mxmrf 1/1 Running 0 31d\rpod/aws-node-njcbd 1/1 Running 0 22d\rpod/aws-node-zpbs2 1/1 Running 0 33d\rpod/coredns-745979c988-5m5n2 1/1 Running 0 22d\rpod/kube-proxy-54grr 1/1 Running 0 22d\rpod/kube-proxy-7hqbf 1/1 Running 0 34d\rpod/kube-proxy-bz4mv 1/1 Running 0 33d\rpod/kube-proxy-rcf8l 1/1 Running 0 31d\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rservice/kube-dns ClusterIP 172.20.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 35d\rNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\rdaemonset.apps/aws-node 4 4 4 4 4 \u0026lt;none\u0026gt; 35d\rdaemonset.apps/kube-proxy 4 4 4 4 4 \u0026lt;none\u0026gt; 35d\rNAME READY UP-TO-DATE AVAILABLE AGE\rdeployment.apps/coredns 1/1 1 1 35d\rNAME DESIRED CURRENT READY AGE\rreplicaset.apps/coredns-745979c988 1 1 1 35d\rBe aware that this command don\u0026rsquo;t really retrieve all resources within the namespace, just few set the resources will be retreived by this command such as: pods, services, daemonsets, deployments, hpa, and replicasets. Other resources such as pv, ingresses, or anything from a Custom Resource Definition won\u0026rsquo;t be shown, in later section we will see a tool that we can use to do deeper inspection about resources installed in a cluster.\nThe kubectl get command allow you to filter data by using label selectors, it does also allows you to change the output format to json, or yaml which will include more information about the resource. For a full list of options and parameter that can be used consult the command help: kubectl get --help.\nkubectl describe This command also allow you to query data about a resource within the kubernetes cluster, this command show details about the specified resource. The describe command is really useful when debugging problems in a cluster as it include data such as \u0026ldquo;events\u0026rdquo; related to the specified resource, or even it will show if the specified resource is linked or controlled by another resource.\nkubectl describe pod -n kube-system -l k8s-app=kube-dns Name: coredns-745979c988-5m5n2\rNamespace: kube-system\rPriority: 2000000000\rPriority Class Name: system-cluster-critical\rNode: ip-10-10-1-209.eu-central-1.compute.internal/10.10.1.209\rStart Time: Tue, 22 Mar 2022 08:18:44 +0000\rLabels: eks.amazonaws.com/component=coredns\rk8s-app=kube-dns\rpod-template-hash=745979c988\rAnnotations: eks.amazonaws.com/compute-type: ec2\rkubernetes.io/psp: eks.privileged\rStatus: Running\rIP: 10.10.1.233\rIPs:\rIP: 10.10.1.233\rControlled By: ReplicaSet/coredns-745979c988\rContainers:\rcoredns:\rContainer ID: docker://f2cd327eb2c65206930da12a863c174432d190817f6f976a0243ad3d5b433f13\rImage: 602401143452.dkr.ecr.eu-central-1.amazonaws.com/eks/coredns:v1.8.4-eksbuild.1\rImage ID: docker-pullable://602401143452.dkr.ecr.eu-central-1.amazonaws.com/eks/coredns@sha256:fcb60ebdb0d8ec23abe46c65d0f650d9e2bf2f803fac004ceb1f0bf348db0fd0\rPorts: 53/UDP, 53/TCP, 9153/TCP\rHost Ports: 0/UDP, 0/TCP, 0/TCP\rArgs:\r-conf\r/etc/coredns/Corefile\rState: Running\rStarted: Tue, 22 Mar 2022 08:19:35 +0000\rReady: True\rRestart Count: 0\rLimits:\rmemory: 170Mi\rRequests:\rcpu: 100m\rmemory: 70Mi\rLiveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5\rReadiness: http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3\rEnvironment: \u0026lt;none\u0026gt;\rMounts:\r/etc/coredns from config-volume (ro)\r/tmp from tmp (rw)\r/var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6tjcg (ro)\rConditions:\rType Status\rInitialized True\rReady True\rContainersReady True\rPodScheduled True\rVolumes:\rtmp:\rType: EmptyDir (a temporary directory that shares a pod's lifetime)\rMedium:\rSizeLimit: \u0026lt;unset\u0026gt;\rconfig-volume:\rType: ConfigMap (a volume populated by a ConfigMap)\rName: coredns\rOptional: false\rkube-api-access-6tjcg:\rType: Projected (a volume that contains injected data from multiple sources)\rTokenExpirationSeconds: 3607\rConfigMapName: kube-root-ca.crt\rConfigMapOptional: \u0026lt;nil\u0026gt;\rDownwardAPI: true\rQoS Class: Burstable\rNode-Selectors: \u0026lt;none\u0026gt;\rTolerations: CriticalAddonsOnly op=Exists\rnode-role.kubernetes.io/master:NoSchedule\rnode.kubernetes.io/not-ready:NoExecute op=Exists for 300s\rnode.kubernetes.io/unreachable:NoExecute op=Exists for 300s\rEvents: \u0026lt;none\u0026gt;\rFrom this output we can get interesting information that can\u0026rsquo;t be retrieved by the get command such as: Node, Controlled By, and a really important one Events.\nTry the following commands and review all data you can get from the describe command:\nkubectl describe node $( kubectl get node -l nodegroup-role=web-workload -o jsonpath='{.items[0].metadata.name}')\rkubectl describe sc gp2\rkubectl describe daemonsets -n kube-system aws-node\r"
},
{
	"uri": "/troubleshooting/workers/",
	"title": "Data Plane - Worker Node Logs",
	"tags": [],
	"description": "",
	"content": "Even though Kubernetes is a distributed platform for running containers, some of the most complex problems in a Kubernetes setup requires to inspect worker nodes joined to the cluster to host Kubernetes pods.\nThere a some important components that will directly impact Kubernetes but Kubernetes itself doesn\u0026rsquo;t provide a mechanism to review them via the standard Kubernetes API, such as: the linux kernel, worker node access audit, daemons executed directly in the worker node (not Kubernets daemonsets), or kubelet itself.\nTo review those components we need to find a way to forward logs from those components to a central location, or access the worker node to get system information. As setting up a full monitoring solution is out of the scope of this workshop, we will focus on how to access or get information directly from the Worker Nodes and what are the most important system logs that have to be seen.\nIn this workshop we assume you are using EKS Worker Nodes based on the EKS Optimized AMI which comes with the SSM Agent already installed.\nHow to access an EKS Worker Node From the Cloud 9 terminal window, let\u0026rsquo;s execute the following commands:\n# Get a Worker Node EC2 ID EC2ID=$(kubectl describe node $( kubectl get node -l nodegroup-role=web-workload -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) | grep ProviderID | cut -d/ -f 5) # Access EC2 instance using SSM Session Manager aws ssm start-session --target $EC2ID # Become root  sudo su - Operative System stats and logs When troubleshooting an EKS Worker Node it is important to collect as much information as possible from the system in order to detect a problem.\nMost common stats we look into when troubleshooting an EKS Worker Node are: CPU/MEM/IO. Linux offer a broad number of tools for inspecting these stats, a thing to take into account is that most of the tools offered by Linux are not \u0026ldquo;container\u0026rdquo; aware and this is normal as there is no such thing as a \u0026ldquo;container\u0026rdquo; resource in the Linux kernel, a \u0026ldquo;container\u0026rdquo; is no more than a regular Linux process that make use of several Kernel features to isolate and control it, some of these Kernel features are: cgroups, namespaces, layered filesystems.\nA pretty common tool to review basic CPU/MEM stats at system wide and process level is the top command, in this workshop we will use atop which is tool that bring same capabilities as top but add more information such as network, and Disk IO stats.\nInstalling and using atop  Enable EPEL repository  sudo amazon-linux-extras install epel Install atop  sudo yum -y install atop --enablerepo=epel Run atop  atop Atop data Atop present data using two different sections, in a similar fashion as the top command, the first section at the top is the system wide stats that show data such as: CPU, MEM, Network, Disk usage. An interesting thing that can be seen is that it is capable to show per CPU, per Network device, or per Disk level metric:\nPRC | sys 0.26s | | user 1.04s | | | #proc 182 | | #trun 3 | #tslpi 915 | | #tslpu 46 | #zombie 0 | | clones 143 | | | #exit 53 | |\rCPU | sys 3% | | user 8% | | irq 0% | | | idle 189% | | wait 0% | | steal 0% | | guest 0% | | | curf 2.30GHz | |\rcpu | sys 2% | | user 4% | | irq 0% | | | idle 94% | | cpu001 w 0% | | steal 0% | | guest 0% | | | curf 2.30GHz | |\r0pu | sys 2% | | user 4% | | irq 0% | | | idle 95% | | cpu001 w 0% | | steal 0% | | guest 0% | | | curf 2.30GHz | |\rCPL | avg1 0.11 | | avg5 0.17 | | avg15 0.17 | | | | | csw 34959 | | intr 24769 | | | | | numcpu 2 | |\rMEM | tot 7.8G | free 1.3G | cache 4.4G | | dirty 0.2M | buff 2.6M | slab 460.3M | slrec 270.3M | | | shmem 2.4M | shrss 0.0M | shswp 0.0M | | | | | numnode 1 |\rSWP | tot 0.0M | | free 0.0M | | swcac 0.0M | | | | | | | | | vmcom 8.3G | | | vmlim 3.9G | |\rDSK | xvda | busy 0% | | read 0 | write 11 | | discrd 0 | KiB/r 0 | | KiB/w 7 | KiB/d 0 | | MBr/s 0.0 | MBw/s 0.0 | | avq 0.00 | avio 0.73 ms | |\rDSK | xvda1 | busy 0% | | read 0 | write 11 | | discrd 0 | KiB/r 0 | | KiB/w 7 | KiB/d 0 | | MBr/s 0.0 | MBw/s 0.0 | | avq 0.00 | avio 0.73 ms | |\rNET | transport | tcpi 341 | | tcpo 356 | udpi 3 | | udpo 3 | tcpao 52 | | tcppo 2 | tcprs 0 | | tcpie 0 | tcpor 5 | | udpnp 0 | udpie 0 | |\rNET | network | | ipi 514 | | ipo 526 | ipfrw 170 | | deliv 344 | | | | | | | icmpi 0 | | icmpo 0 | |\rNET | eth0 0% | | pcki 282 | | pcko 293 | | sp 10 Gbps | si 51 Kbps | so 92 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eth1 0% | | pcki 15 | | pcko 26 | | sp 10 Gbps | si 4 Kbps | so 1 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni2181 ---- | | pcki 39 | | pcko 41 | | sp 0 Mbps | si 5 Kbps | so 9 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | enif921 ---- | | pcki 5 | | pcko 5 | | sp 0 Mbps | si 10 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni5d45 ---- | | pcki 46 | | pcko 39 | | sp 0 Mbps | si 4 Kbps | so 3 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni0b2d ---- | | pcki 30 | | pcko 30 | | sp 0 Mbps | si 3 Kbps | so 2 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni5d94 ---- | | pcki 17 | | pcko 13 | | sp 0 Mbps | si 1 Kbps | so 3 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | enid97f ---- | | pcki 16 | | pcko 14 | | sp 0 Mbps | si 1 Kbps | so 3 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | lo ---- | | pcki 42 | | pcko 42 | | sp 0 Mbps | si 2 Kbps | so 2 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni9360 ---- | | pcki 14 | | pcko 13 | | sp 0 Mbps | si 1 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni8ef2 ---- | | pcki 4 | | pcko 6 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | enib4ef ---- | | pcki 2 | | pcko 3 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | enibc18 ---- | | pcki 2 | | pcko 1 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni2acd ---- | | pcki 1 | | pcko 2 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni62ac ---- | | pcki 2 | | pcko 2 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rNET | eni56f4 ---- | | pcki 2 | | pcko 2 | | sp 0 Mbps | si 0 Kbps | so 0 Kbps | | | coll 0 | mlti 0 | erri 0 | erro 0 | | drpi 0 | drpo 0 |\rThe bottom part will show per process metrics, by default it will show a generic view, in this workshop will use only this view.\n PID CID SYSCPU USRCPU RDELAY VGROW RGROW RDDSK WRDSK RUID EUID ST EXC THR S CPUNR CPU CMD 1/5\r7478 host-------- 0.46s 1.10s 0.84s 0B 548.0K 0B 0B root root -- - 15 S 0 5% kubelet\r7164 host-------- 0.09s 0.70s 0.36s 0B 0B 0B 32.0K root root -- - 32 S 0 3% dockerd\r14170 78f0ab262c2d 0.00s 0.12s 0.07s 0B 0B 0B 48.0K chrony chrony -- - 34 S 0 0% mongod\r13887 56ee1595fdc1 0.00s 0.11s 0.00s 0B -64.0K 0B 20.0K chrony chrony -- - 35 S 1 0% mongod\r15481 2f7d7a0a9ceb 0.00s 0.10s 0.05s 0B 0B 0B 0B chrony chrony -- - 70 S 1 0% beam.smp\r9382 ae8b43e9f199 0.02s 0.08s 0.37s 0B 0B 0B 0B nfsnobod nfsnobod -- - 8 S 0 0% node_exporter\r7072 host-------- 0.01s 0.07s 0.13s 0B 0B 0B 0B root root -- - 15 S 0 0% containerd\r17290 host-------- 0.03s 0.03s 0.00s -0.9M -1.0M 0B 0B root root -- - 1 R 0 0% atop\r13496 f2cd327eb2c6 0.00s 0.05s 0.02s 0B 0B 0B 0B root root -- - 11 S 1 0% coredns\r609 host-------- 0.02s 0.03s 0.01s 0B 0B 0B 8.0K root root -- - 9 S 1 0% ssm-session-wo\r13915 220d0c701f13 0.01s 0.02s 0.00s 0B 0B 0B 0B 10001 10001 -- - 27 S 0 0% java\r14008 820dbee6c64c 0.01s 0.02s 0.00s 0B 0B 0B 4.0K root root -- - 30 S 1 0% java\r4707 f525fb74eada 0.01s 0.02s 0.02s 0B 0B 0B 0B 65532 65532 -- - 9 S 0 0% controller\rIn this section it is interesting to see the column CID this column is actually showing the Container ID of a process, if the process is not running within a Docker container then this column will show host instead.\nIf you\u0026rsquo;d like to know more advanced usage of atop you can see this AWS Premium Support article https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-monitor-stats-with-atop/\nOther useful tools Other useful tools to collect system data for troubleshooting are:\n ss for network connections  ss -atup  lsof to review open files per processes  lsof -p \u0026lt;PID\u0026gt;  iptables to review firewall and routing rules  # iptables rules\riptables -nL -v\r# iptables rules in NAT table\riptables -nL -v -t nat\r# iptables rules added by Kubernetes services\riptables -t nat -L KUBE-SERVICES -n -v\rSystem Logs As most Linux distributions all operative system logs can be seen in directory /var/log, specific files for operative system logs are messages, secure, dmesg, boot.log. As Amazon Linux 2 uses systemd then you can use also journalctl to view system logs:\n# General linux logs\rjournalctl # Kernel specific logs\rjournalctl -k\rkubelet logs To see kubelet status you must run the following command:\nsystemctl status kubelet Check if kubelet is listening for connections\nss -lpn | grep $(pidof kubelet) Check list of open files by kubelet\nlsof -p $(pidof kubelet) Check kubelet logs:\njournalctl -u kubelet docker daemon logs To see docker status you must run the following command:\nsystemctl status docker Check if docker is listening for connections\nss -lpn | grep $(pidof dockerd) Check list of open files by docker\nlsof -p $(pidof dockerd) Check docker logs:\njournalctl -u docker containerd logs To see containerd status you must run the following command:\nsystemctl status containerd Check if containerd is listening for connections\nss -lpn | grep $(pidof containerd) Check list of open files by containerd\nlsof -p $(pidof containerd) Check containerd logs:\njournalctl -u containerd IPAM daemon logs The IP address management plugin is a component used by the CNI to maintain a warm-pool of available secondary IP addresses, logs generated by this plugin are stored locally in the Worker Node host so it is not possible to view them using the kubectl logs command, logs generated by this component are stored at /var/log/aws-routed-eni and provide detailed information about how the CNI is assigning IP addresses to Pods started in that specific Worker Node. So, if you need to troubleshoot CNI issues then these are the logs you are looking for.\ntail -n 100 /var/log/aws-routed-eni/ipamd.log tail -n 100 /var/log/aws-routed-eni/plugin.log Collecting data from multiple nodes There are times when complex troubleshooting needs to be performed and you need to collect data from multiple or even all worker nodes for deeper inspection. Fortunately, as the EKS Optimized AMI comes with the SSM agent already installed this is an easy task.\nAWS SSM Run Command If you need to run a command in multiple nodes and get the result of the command output then you can use AWS SSM Run Command for this task.\n Let\u0026rsquo;s check the Worker Nodes are successfully associated to SSM  aws ssm describe-instance-information \\  --filters \u0026#34;Key=tag-key,Values=kubernetes.io/cluster/troubleshoot-workshop\u0026#34; If previous command returned a list of instances, then you can use the following command to send the execution to all registered instances  aws ssm send-command \\  --targets \u0026#34;Key=tag:kubernetes.io/cluster/troubleshoot-workshop,Values=owned\u0026#34; \\  --document-name \u0026#34;AWS-RunShellScript\u0026#34; \\  --parameters \u0026#34;commands=curl http://localhost:61679/v1/enis | python -m json.tool\u0026#34; \\ \t--output-s3-bucket-name ${S3_BUCKET} \\ \t--output-s3-key-prefix \u0026#34;/\u0026#34; View command output 3.1. Access System Manager console 3.2. Access Run Command Console 3.3. Click command history tab 3.4. Click the command ID 3.5. Click on one of the Instances ID and expand the Output section   At this stage you should also have all command outputs stored in the S3 bucket we specified in the send-command parameters.\nAWS SSM Premium Support document The AWS Premium Support team has prepared the \u0026ldquo;AWSSupport-CollectEKSInstanceLogs\u0026rdquo; AWS SSM Document to collect most important data you need to collect from an EKS worker node for troubleshooting, you can use this document at anytime for your own troubleshooting.\nRun the following command to execute the automation:\naws ssm start-automation-execution \\ \t--document-name \u0026#34;AWSSupport-CollectEKSInstanceLogs\u0026#34; \\ \t--document-version \u0026#34;\\$DEFAULT\u0026#34; \\ \t--parameters \u0026#34;{\\\u0026#34;EKSInstanceId\\\u0026#34;:[\\\u0026#34; \\\u0026#34;],\\\u0026#34;LogDestination\\\u0026#34;:[\\\u0026#34;${S3_BUCKET}\\\u0026#34;]}\u0026#34; \\ \t--target-parameter-name EKSInstanceId --targets \u0026#39;[{\u0026#34;Key\u0026#34;:\u0026#34;tag:kubernetes.io/cluster/troubleshoot-workshop\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;owned\u0026#34;]}]\u0026#39; \\ \t--max-errors \u0026#34;1\u0026#34; \\ \t--max-concurrency \u0026#34;100%\u0026#34; To see the logs you should open the S3 console and open the S3 bucket provided to the command, you should see several files like this one eks_i-049409aed777a71bb_fb9ff9e5-6011-4a89-a2e2-c7c1fb24b7d7.tar.gz. Download it and open the .tar.gz file.\n"
},
{
	"uri": "/troubleshooting/dpmetrics/",
	"title": "Data Plane - Metrics",
	"tags": [],
	"description": "",
	"content": "Metrics are mostly numerical data points measured over intervals of time forming a time series. Metrics are an essential source for troubleshooting problems in your infrastructure, but reviewing raw metrics is not useful, the right way to investigate metrics is by keeping historical data of metrics. So, it will let you know changes in the behaviour of your platform and applications. For this reason it is important to store metrics in a timeseries database.\nSetting a full monitoring solution is out of the scope of this workshop. So, we are not going to dig deeper on metrics, for more information about how to setup a monitoring solution to injest EKS metrics you can see the following resources:\n Monitoring using Prometheus and Grafana One Observability Workshop  Even though we are not going to go deeper into this topic, it is important to mention that beside common infrastructure metrics such as: CPU, MEM, Networking. Every component we install in our EKS cluster is capable to generate it own metrics and it is quite important to injest those metrics for later inspection, here some examples:\n kube-proxy kubelet CoreDNS Worker Nodes Cluster Autoscaler Nginx Ingress Controller  As you can notice from the documentation of every component, all of them expose metrics in Prometheus format. So, it is quite important that no matter what monitoring solution you will use it have to support some sort of Prometheus support.\nIf you decide to use Prometheus as monitoring solution, then an easy way to setup a monitoring solution is by using the kube-prometheus-stack helm chart, this chart will install all required tools and components to have a full prometheus setup, but also it comes with a set of pre-installed Grafana dashboards and alert manager rules that will monitor basic Kubernetes components.\n"
},
{
	"uri": "/troubleshooting/cpmetrics/",
	"title": "Control Plane - Metrics",
	"tags": [],
	"description": "",
	"content": "EKS Control Plane has been designed for scale and high availability, it is capable of handling large clusters. For the vast majority of customers and use cases the control plane will scale to handle the Kubernetes requests. However in certain scenarios such as a sudden extreme increase in Kubernetes API traffic, or spiky scaling workloads where the cluster may be idle for long times, can impact control plane performance.\nEvaluating the scalability of a Kubernetes control plane is a complex topic and there are a number of factors from that can impact the performance of a cluster. Everything from the configuration of the Kubernetes control plane components, custom add-ons that are installed, to the behavior of the nodes and pods inside the cluster can change the way a control plane behaves.\nEKS Control Plane metrics are exposed by default for every EKS cluster, these metrics are exposed as Prometheus metrics. In order to collect these metrics you must use an agent capable of parsing Prometheus metrics and ingesting them into your monitoring platform\nKey Control Plane Metrics rest_client_requests_total: This metric represent total number of HTTP requests made to the API server, partitioned by status code, method, and host. This metric is important to detect anomalies or sudden changes in Kubernetes behavior. The API server request behavior depends on multiple factors, such as: cluster size, type of workload running in the cluster, configuration of kubernetes clients, etc. For this reason it is difficult to set a specific threshold that would indicate problems, however you can review this metric for large increases or decreases which could indicate problems.\n A large unexpected increase in the total requests to the API server could indicate that some of the components used in the cluster are querying the API Server inappropriately and should be further investigated. A large unexpected decrease on total requests to the API server could be an indication that some of the components deployed in your cluster are not capable of reaching the API Server or have encountered another problem and have stopped communicating.  It is important to notice that this metric also include de status code of a requests. So, you can easily monitor the number of errors returned by the API Server.\napiserver_current_inqueue_requests: Number of queued requests in API Server per request kind in last second.\napiserver_current_inqueue_requests{instance=\u0026quot;10.10.1.176:443\u0026quot;, job=\u0026quot;apiserver\u0026quot;, request_kind=\u0026quot;mutating\u0026quot;} 1\rAbove example show a metric datapoint which mean that 1 request is in the queue in the last second and it is mutating operation kind. This metric can be correlated total number of requests. So, we can detect when the number of requests sent to the control plane is too high to be handled.\napiserver_current_inflight_requests: Number of API calls being processed by the API server in a given time. This metric is part of the API server control flow feature that was added to Kubernetes in version 1.18 and introduced into EKS in version 1.20\nMore information about Kubernetes API server control flow at https://kubernetes.io/docs/concepts/cluster-administration/flow-control/\napiserver_request_duration_seconds: Will return time series data about API Server latency, how long it took the API server to handle the request. A high value for a consistent period of time means that EKS Control Plane is having a hard time returning responses in a timely manner, this could indicate a large amount of requests or that a bottleneck has been encountered.\nSome latency in is to be expected, particularly on calls that mutate objects in the Kubernetes cluster or when requesting a large amount of data. As the latency on requests grows longer, it will take longer for changes to be applied in the cluster and utilities that build a “state of the world” like logging/monitoring agents may be slow to start. If the latency value climbs high enough the Kubernetes API server will timeout the request and this in turn will cause delays and errors in your cluster.\netcd_request_duration_seconds: ETCD latency, this metrics returns time series data about the requests from the API server to etcd*,* this metric is good to observe mostly to correlate against apiserver_requests_duration_seconds*.\nLatency from etcd will directly impact API server latencies, this metric can help determine if the latency is being introduced when retrieving the data from etcd or when the API server processes the response.\netcd_object_counts: Returns number of kubernetes objects stored in ETCD. This metric is important to review Kubernetes usage pattern. As mentioned at the beginning Kubernetes scalability and performance depends not only on the number of Worker Nodes and/or Pods but the total number of Objects stored in the cluster and its usage. For example you may notice a spike in requests and latencies reported for the API server metrics, this metric could help you see there was a pod scaling event that would have driven the increase in requests.\napiserver_requested_deprecated_apis: This metric return the number of deprecated APIs being used in the cluster, this metric is useful when planning cluster upgrades as you should migrate your application to not use deprecated APIs.\n"
},
{
	"uri": "/troubleshooting/cplogs/",
	"title": "Control Plane - Logs",
	"tags": [],
	"description": "",
	"content": "The EKS Control Plane is able to deliver logs to CloudWatch log groups in your account, this is not enabled by default as it does imply additional costs for CloudWatch Logs storage and ingestion. To enable EKS Control Plane logs follow this instructions: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html\nStrictly speaking of troubleshooting the most important logs to detect problems in Kubernetes control plane are:\n  Kubernetes API server component logs (api) – Your cluster\u0026rsquo;s API server is the control plane component that exposes the Kubernetes API. Useful to detect problems related to malformed requests, API server latency and timeout, Control plane unable to process request.\n  Audit (audit) – Kubernetes audit logs provide a record of the individual users, administrators, or system components that have affected your cluster. Useful to deep dive into components doing heavy usage of Kubernetes Control Plane or trace changes.\n  Controller manager (controllerManager) – The controller manager manages the core control loops that are shipped with Kubernetes. Useful to detect reconciliation problems or failures on controllers execution.\n  Scheduler (scheduler) – The scheduler component manages when and where to run pods in your cluster. Useful to detect if Kubernetes scheduler isn’t capable to allocate pods in the Data Plane.\n  Control Plane logs are an excelent source for troubleshooting issues in our Kubernetes cluster, specially if we are looking to troubleshoot possible control plane issues. EKS Control Plane logs can be queried via Cloudwatch Logs Insights.\nAccess the Cloudwatch Logs Insights console and select the log group “/aws/eks/\u0026lt;CLUSTER_NAME\u0026gt;/cluster”.\nHere a list of useful Cloudwatch Logs Insights queries that can be used to troubleshoot EKS control plane issues.\n List apiserver 5XX by URL, Verb, HTTP Code, and User Agent  stats count(*) as count by requestURI, verb, responseStatus.code, userAgent | filter @logStream =~ \u0026#34;kube-apiserver-audit\u0026#34; | filter responseStatus.code \u0026gt;= 500 | sort count desc | limit 1000  List apiserver 5XX by URL, Verb, HTTP Code, and User Agent  stats count(*) as count by requestURI, verb, responseStatus.code, user.username | filter @logStream =~ \u0026#34;kube-apiserver-audit\u0026#34; | filter responseStatus.code \u0026gt;= 500 | sort count desc | limit 1000  Slowest API Calls  fields @timestamp, @message | filter @logStream =~ \u0026#34;kube-apiserver-audit\u0026#34; | filter ispresent(requestURI) | filter verb = \u0026#34;list\u0026#34; | parse requestReceivedTimestamp /\\d+-\\d+-(?\u0026lt;StartDay\u0026gt;\\d+)T(?\u0026lt;StartHour\u0026gt;\\d+):(?\u0026lt;StartMinute\u0026gt;\\d+):(?\u0026lt;StartSec\u0026gt;\\d+).(?\u0026lt;StartMsec\u0026gt;\\d+)Z/ | parse stageTimestamp /\\d+-\\d+-(?\u0026lt;EndDay\u0026gt;\\d+)T(?\u0026lt;EndHour\u0026gt;\\d+):(?\u0026lt;EndMinute\u0026gt;\\d+):(?\u0026lt;EndSec\u0026gt;\\d+).(?\u0026lt;EndMsec\u0026gt;\\d+)Z/ | fields (StartHour * 3600 + StartMinute * 60 + StartSec + StartMsec / 1000000) as StartTime, (EndHour * 3600 + EndMinute * 60 + EndSec + EndMsec / 1000000) as EndTime, (EndTime - StartTime) as DeltaTime | stats avg(DeltaTime) as AverageDeltaTime, count(*) as CountTime by requestURI, user.username | filter CountTime \u0026gt;= 10 | sort AverageDeltaTime desc  List LIST operations per user Agent  fileds @timestamp, @message, @logStream | filter @logStream =~ \u0026#34;kube-apiserver-audit\u0026#34; | filter verb = \u0026#34;list\u0026#34; | filter objectRef.resource = \u0026#34;pods\u0026#34; | filter requestURI like /\\/api\\/v1\\/pods/ | stats count(*) as cnt by userAgent | sort cnt desc  List anonymous requests  fields @timestamp, @message, sourceIPs.0 | sort @timestamp desc | limit 100 | filter user.username=\u0026#34;system:anonymous\u0026#34;  List top mutable calls  fields @timestamp, @message, @logStream, requestURI, verb | filter @logStream =~ \u0026#34;kube-apiserver-audit\u0026#34; | filter verb not like \u0026#34;get\u0026#34; | filter verb not like \u0026#34;list\u0026#34; | filter verb not like \u0026#34;watch\u0026#34; | display @logStream, requestURI, verb | stats count(*) as count by requestURI, verb | sort count desc  List failed API server health checks  fields @message | sort @timestamp asc | filter @logStream like \u0026#34;kube-apiserver\u0026#34; | filter @logStream not like \u0026#34;kube-apiserver-audit\u0026#34; | filter @message like \u0026#34;healthz check failed\u0026#34; | limit 100 "
},
{
	"uri": "/microservice/",
	"title": "Installing Addons and Microservices",
	"tags": [],
	"description": "",
	"content": "From this section we will start installing components in our cluster, during the whole installation process we will get presented with a different set of issues that we have to troubleshoot and fix.\nIn order to install all required components we need to clone the workshop repository\ngit clone https://github.com/overdrive3000/troubleshoot-workshop.git By the end of this section we should have deployed a functional cluster with an example microservice installed in it.\n"
},
{
	"uri": "/microservice/addons/",
	"title": "Installing Addons",
	"tags": [],
	"description": "",
	"content": "In this section let\u0026rsquo;s install a serie of Addons we need to have a fully functional and scalable cluster.\n Change to the repository folder  cd troubleshoot-workshop Access the addons folder and execute the apply.sh script  cd addons/infra bash ./apply.sh helm ls --all-namespaces Exercise You have 10 minutes to review all components installed and verify everything is correct. In this exercise you should make use of the different techniques we have discussed previously such as get and describe resources, and checking components logs.\nHave you found an error?\n"
},
{
	"uri": "/microservice/tools/",
	"title": "Installing Troubleshooting Tools",
	"tags": [],
	"description": "",
	"content": "I previous version we were able to perform some trouebleshooting in our cluster, but we could notice that performing troubleshooting just using kubectl it could be cumbersome.\nIn this section we will install some tools that we help us to inspect our cluster resources in an easy way, let\u0026rsquo;s start by installing some plugins for kubectl that will enhance the cli features.\nInstalling kubectl plugins  Installing krew  ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; OS=\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)\u0026#34; \u0026amp;\u0026amp; ARCH=\u0026#34;$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/\u0026#39; -e \u0026#39;s/aarch64$/arm64/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; KREW=\u0026#34;krew-${OS}_${ARCH}\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf \u0026#34;${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; ./\u0026#34;${KREW}\u0026#34; install krew ) export PATH=\u0026#34;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026#34; cd ${HOME}/environment/troubleshoot-workshop Install some useful plugins  kubectl krew install stern kubectl krew install deprecations kubectl krew install get-all k9s k9s is a Terminal User Interface for Kubernetes that make easier to explore your cluster and perform basic troubleshooting tasks such as: viewing cluster resources, pod logs, port forwarding, etc.\nTo install k9s follow the steps described at their official documentation https://k9scli.io/topics/install/\nFor this workshop k9s is already installed in our Cloud9 environment. So, to use it just open the Cloud9 shell and run:\nk9s kube-ops-view kube-ops-view is a simple yet useful tool to visualize the status of nodes and pods in our cluster, the way it present the data is interesting as you can easily watch how our pods are distributed across the cluster. Information about the project can be found here https://codeberg.org/hjacobs/kube-ops-view\nIn this workshop kube-ops-view is already installed in our environment to access it, execute the following steps in the Cloud9 shell:\n Get load balancer information from kube-ops-view service  kubectl get service -n monitoring kube-ops-view -o jsonpath={.status.loadBalancer.ingress[0].hostname} \u0026amp;\u0026amp; echo In your own laptop open the Load Balancer URL from previous step in your browser.  Prometheus / Grafana stack Even though we will not dive deep into setting up a monitoring stack for EKS, we have installed a basic Prometheus/Grafana stack in order to help us to troubleshoot the different cases we will face during this workshop. To access our Grafana UI run following steps:\n Get load balancer information from grafana service  kubectl get service -n monitoring kube-prometheus-stack-grafana -o jsonpath={.status.loadBalancer.ingress[0].hostname} \u0026amp;\u0026amp; echo In your own laptop open the Load Balancer URL from previous step in your browser. User and password we will provided in classroom.  Optional tooling Installing Lens (Optional) Lens is a graphic client for Kubernetes, this tools has to be installed locally. So, for that reason you need to installing it in your own laptop instead of the Cloud9 shell. In order to make it work with our cluster there are some pre-requisits such as:\n Install the AWS CLI https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html Install kubectl https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html Configure the AWS CLI creating a new profile, let\u0026rsquo;s name this profile workshop https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html You can get AWS CLI credentials for our environmnet here Install lens https://docs.k8slens.dev/main/getting-started/  Enable Lens metrics Once you have connected to the cluster, let´s perform following steps:\n Click on cluster name to expand the menu  Click on Settings Click on Extenssions \u0026gt; Lens Metrics  Enable all options and click Apply button   Install mizu Mizu offers a real-time view of all HTTP requests, REST and gRPC API calls, as well as Kafka, AMQP (activeMQ / RabbitMQ), and Redis. This tool will present all traffic capture in a visual interface with advanced filter mechanisms that will allow us to narrow down the traffic we are looking for.\nMizu relies on kubectl configuration to connect to the cluster, so at this stage you only need to install mizu binaries in your machine. Follow instructions from https://up9.com/docs/open-source-tools/mizu/installing-mizu\n This tool is already installed in our Cloud9 environmen.\n "
},
{
	"uri": "/microservice/microservice/",
	"title": "Installing Sock Shop Microservice",
	"tags": [],
	"description": "",
	"content": "In this workshop we will use the popular sock-shop demo microservice from Weaveworks https://microservices-demo.github.io/.\nThe main goal of this section is to have properlly working the sock-shop application, as in next sections we will start adding some traffic to it.\nInstall the microservice  Change to the workshop folder  cd ${HOME}/environment/troubleshoot-workshop Deploy the microservice manifest  kubectl apply -f sock-shop/deploy.yaml Exercise Review that the microservice has been deployed properlly and you can access the Microservice front end. Make use of the tools we have just installed to make troubleshoot easier. You have 20 min to identify as most problems as possible from current setup, try to meet all goals described below.\nGoals  Without looking at the manifest try to recognize all resources installed by the manifest. Is the application up? Can you identify issues in the application? Can you see how to access the application from your browser? Is the application deploying additional AWS resources? Are all those resources created and linked to the application?  Concepts After perform an initial troubleshooting we will discuss the following Kubernetes topics in advance.\n The Kubernetes Scheduler  Taints and Tolerations QoS and Priorities   Kubernetes Controllers and Reconciliation Kubernetes Services and Endpoints The AWS Load Balancer Controller  Resources  Kubernetes Taints and Tolerations Pod Preemption Node Pressure Eviction Kubernetes Services Deep Dive  "
},
{
	"uri": "/traffic/",
	"title": "Injecting Traffic",
	"tags": [],
	"description": "",
	"content": "In this section we will inject traffic to our application to see how the application is capable of handle traffic. By the end of this section you will know how to inspect live traffic within a Kubernetes cluster and how to solve most common scalability problems.\n"
},
{
	"uri": "/traffic/simulator/",
	"title": "Installing Traffic Simulation Client",
	"tags": [],
	"description": "",
	"content": "In order to demonstrate how to troubleshoot problems in live environments, we will deploy a client outside of our EKS cluster that will hit our application to simulate traffic. To do this we will deploy a simple ECS Fargate cluster that will run a locust application performing few actions in our Sock Shop application, this Fargate cluster we will deployed in another VPC in order to simulate real traffic as much as possible.\nDeploying client application  Change to the client application folder  cd ${HOME}/environment/troubleshoot-workshop/simulate-traffic Open config.ts file and fill the LoadBalancerURL parameter value with the URL of the Sock Shop microservice demo Load Balancer, save and close this file. Deploy the client by running:  cdk deploy --require-approval=never --all "
},
{
	"uri": "/traffic/trafficview/",
	"title": "Inspect Microservice traffic",
	"tags": [],
	"description": "",
	"content": "Application Load Balancer metrics The easiest way to review if our microservice is receiving traffic from outside world is by checking Cloudwatch metrics for the entrypoint load balancer managed by the Ingress controller.\n Open the AWS EC2 console Click Target Groups at the left menu  Select the Target Group corresponding to the front end microservice  At the bottom, click on the Monitoring tab   These metrics will let us know how our microservice is handling external connections. Key metrics to look at are:\n Requests HTTP 2XXs Target Response Time HTTP 5XXs  As explained earlier, metrics are an useful data source to quickly detect if our application is misbeahaving. In the case you see an elevated number of 5xx error then you must investigate further, the best data source to investigate specific problems are logs. In this case you can check the front-end microservice logs to start your investigation.\nIn a production environment with dozen or thousands of microservices, rely only on metrics and logs is not enough for rapid troubleshooting as you need to follow the path of your packages to exactly know where the problem is. Full observability stacks also relies on traces in order to get a clear view of how our microservices talk to each other. Traces are out of the scope of this workshop, if you want to learn more about this topic then take a look to the One Observability Workshop.\nMizu The harder part of troubleshooting network issues in Kubernetes is to inspect network traffic once it ingress into the cluster network. Fortunately, the Kubernetes community is working really hard on making this task simpler. Mizu is an opensource tool that allow us to inspect real time L7 traffic in our cluster. So, we can see all communication between the different pods within the cluster.\nTo run mizu let\u0026rsquo;s run the following commands in our Cloud9 environment:\n Run mizu  mizu tap \u0026#34;(.*)\u0026#34; --namespaces sock-shop -p 8080 In Cloud9 click on the Preview button  Click on Preview Running Application   At this point you should be able to see the mizu web interface that is showing real time traffic between all pods in the sock-shop namespace.\nMizu allow to filter traffic seen in the console by using a pretty simple query language, here some examples:\n Filter out health check probes  request.headers[\u0026#34;User-Agent\u0026#34;] != \u0026#34;kube-probe/1.21+\u0026#34;  Look for all 5xx status codes  response.status \u0026gt;= 500  Look for all requests to an specific path  http and request.path == \u0026#34;/orders\u0026#34; "
},
{
	"uri": "/traffic/exercise/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": "Part 1 At this stage you have the microservice running and a set of clients using your application, but in last section you should\u0026rsquo;ve noticed that the application is throwing a considerable number of 5xx errors. You have 15 minutes to fix most of the 5xx errors.\nGoal  Fix most of the 5xx errors  Part 2  Have you fixed 5xx errors? What if we increment the application traffic?  Execute the follwoing steps:\n Change to the client application folder  cd ${HOME}/environment/troubleshoot-workshop/simulate-traffic Open config.ts file and update the NumReplicas to 20. Deploy the client by running:  cdk deploy --require-approval=never --all You have 15 minutes to meet the following goals\nGoals  Check if traffic in our application has incremented Check if the application is throwing 5xx erros again Make the microservice scalable  Concepts  Pod sizing (limits and requests) CPU Throttling Out of Memory in Pods Kubernetes metrics endpoints Horizontal Pod Autoscaler  Resources  Setting Pod Resources Linux CPU Throttling Kubernetes Resource Metrics Horizontal Pod Autoscale  "
},
{
	"uri": "/extras/",
	"title": "Extras",
	"tags": [],
	"description": "",
	"content": "In this section we will inject some erros by using the Litmus Chaos Engineering Framerwork and show some techniques to inspect problems in a Kubernetes cluster, in addition we will see a quick view of Pixie which is an advanced observability tool that provides an excellent set of features to troubleshoot complex issues in a Kubernetes cluster.\n"
},
{
	"uri": "/extras/exec/",
	"title": "Debug running pods",
	"tags": [],
	"description": "",
	"content": "Sometimes when debugging problems in our applications it is required to access the application container to have a better understanding about what is going on. The kubectl command offer an interface to execute remote commands in our containers, you can even get a shell within the container, in order to do that you need a shell installed within the container image.\nThe Kubernetes documentation have an excellent documentation about how to use kubectl exec to debug applications https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#container-exec. The main problem with this technique is that for troubleshooting you most likely are going to need a set of command line tools, such as: nslookup, or curl. If those tools are not installed in the container image then you need to install them in runtime, for that you need root access to your container which is not a security best practice.\nThe Kubernetes project is aware of this limitation and is working in a new feature that will allow you to inject a \u0026ldquo;debug\u0026rdquo; ephemeral container into your pods. So, you can execute whatever tool you need from that container while sharing the same environment as your running application. The kubectl debug is not yet available in EKS as the feature is still in alpha feature in Kubernetes 1.22.\nHands On Lab In this lab we are going to simulate what the kubectl debug command does but using a different approach:\n Get Litmus Framework URL and open it in a browser  echo \u0026#34;http://$(kubectl get service -n litmus litmus-frontend-service -o jsonpath={.status.loadBalancer.ingress[0].hostname}):9091\u0026#34;  Note: User and Password will be provided in classroom\n Init litmus config  litmusctl config set-account --endpoint \u0026#34;http://$(kubectl get service -n litmus litmus-frontend-service -o jsonpath={.status.loadBalancer.ingress[0].hostname}):9091\u0026#34; --password \u0026lt;PASSWORD\u0026gt; --username \u0026#34;workshop\u0026#34;  Note: Use password you set in Litmus portal\n 2.1. Install litmus agent\nlitmusctl create agent --agent-name=\u0026quot;workshop\u0026quot; --non-interactive --project-id=\u0026lt;PROJECT_ID\u0026gt;\rInstall Litmus experiments  cd ~/environment/troubleshoot-workshop/failures/ ./setup.sh Inject failure  cd ~/environment/troubleshoot-workshop/failures/failure-00 ./apply.sh Get user microservice pod  kubectl -n sock-shop get pod -l name=user Access user microservice pod shell  kubectl -n sock-shop exec -ti \u0026lt;POD_NAME\u0026gt; -- /bin/sh Once in the user microservice container run:  / $ id\ruid=10001(myuser) gid=10001(mygroup) groups=10001(mygroup)\r/ $ dig\rcommand not found\rDeploy debug deployment  kubectl apply -f user-debug.yaml Wait few minutes and access new debug pod  kubectl exec -n sock-shop -it \u0026lt;USER_DEBUG_POD_NAME\u0026gt; -c debug -- /bin/bash\rbash-5.1# id\ruid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)\rbash-5.1# dig amazon.com\r; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.22 \u0026lt;\u0026lt;\u0026gt;\u0026gt; amazon.com\r;; global options: +cmd\r;; Got answer:\r;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 16615\r;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1\r;; OPT PSEUDOSECTION:\r; EDNS: version: 0, flags:; udp: 4096\r; COOKIE: 4729382a84fe13e2 (echoed)\r;; QUESTION SECTION:\r;amazon.com. IN A\r;; ANSWER SECTION:\ramazon.com. 25 IN A 176.32.103.205\ramazon.com. 25 IN A 205.251.242.103\ramazon.com. 25 IN A 54.239.28.85\r;; Query time: 3 msec\r;; SERVER: 172.20.0.10#53(172.20.0.10)\r;; WHEN: Wed Apr 20 17:40:09 UTC 2022\r;; MSG SIZE rcvd: 129\rbash-5.1# tcpdump\rClean up  kubectl delete -f user-debug.yaml Bonus case Debugging an Addon.\n Access to the Amazon Load Balancer pod shell  kubectl get pod -n infrastructure -l app.kubernetes.io/name=aws-load-balancer-controller\rNAME READY STATUS RESTARTS AGE\raws-load-balancer-controller-765558d4c8-4lpqv 1/1 Running 0 5h19m\r$ kubectl exec -ti -n infrastructure aws-load-balancer-controller-765558d4c8-4lpqv -- /bin/sh\rOCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \u0026quot;/bin/sh\u0026quot;: stat /bin/sh: no such file or directory: unknown\rcommand terminated with exit code 126\rAs seen in previous step, it is not possible to get the Amazon Load Balancer controller shell as its Docker image doesn\u0026rsquo;t include a shell.\nDeploy the controller debug deployment  kubectl apply -f controller-debug.yaml Access the debug controller pod  kubectl get pod -n infrastructure -l app.kubernetes.io/name=aws-load-balancer-controller\rNAME READY STATUS RESTARTS AGE\raws-load-balancer-controller-765558d4c8-4lpqv 1/1 Running 0 5h22m\raws-load-balancer-controller-debug-7dc9dc9557-xcs2z 2/2 Running 0 48s\rkubectl exec -ti -n infrastructure aws-load-balancer-controller-debug-7dc9dc9557-xcs2z -c debug -- /bin/bash\rbash-5.1# id\ruid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video),65534(nobody)\rInstall the awscli within the debug pod  pip install awscli Perform some aws cli call to test pod IAM permissions  bash-5.1# aws s3 ls An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied\rbash-5.1# aws elbv2 describe-target-groups\r{\r\u0026quot;TargetGroups\u0026quot;: [\r{\r\u0026quot;TargetGroupArn\u0026quot;: \u0026quot;arn:aws:elasticloadbalancing:eu-west-1:325483425329:targetgroup/k8s-sockshop-frontend-669a180974/14d18a7e41d74c33\u0026quot;,\r\u0026quot;TargetGroupName\u0026quot;: \u0026quot;k8s-sockshop-frontend-669a180974\u0026quot;,\r\u0026quot;Protocol\u0026quot;: \u0026quot;HTTP\u0026quot;,\r\u0026quot;Port\u0026quot;: 8079,\r\u0026quot;VpcId\u0026quot;: \u0026quot;vpc-00563c945e3d9e376\u0026quot;,\r\u0026quot;HealthCheckProtocol\u0026quot;: \u0026quot;HTTP\u0026quot;,\r\u0026quot;HealthCheckPort\u0026quot;: \u0026quot;traffic-port\u0026quot;,\r\u0026quot;HealthCheckEnabled\u0026quot;: true,\r\u0026quot;HealthCheckIntervalSeconds\u0026quot;: 15,\r\u0026quot;HealthCheckTimeoutSeconds\u0026quot;: 5,\r\u0026quot;HealthyThresholdCount\u0026quot;: 2,\r\u0026quot;UnhealthyThresholdCount\u0026quot;: 2,\r\u0026quot;HealthCheckPath\u0026quot;: \u0026quot;/\u0026quot;,\r\u0026quot;Matcher\u0026quot;: {\r\u0026quot;HttpCode\u0026quot;: \u0026quot;200\u0026quot;\r},\r\u0026quot;LoadBalancerArns\u0026quot;: [\r\u0026quot;arn:aws:elasticloadbalancing:eu-west-1:325483425329:loadbalancer/app/k8s-sockshop-frontend-d289a1782e/427a6e79b70db0d6\u0026quot;\r],\r\u0026quot;TargetType\u0026quot;: \u0026quot;ip\u0026quot;,\r\u0026quot;ProtocolVersion\u0026quot;: \u0026quot;HTTP1\u0026quot;,\r\u0026quot;IpAddressType\u0026quot;: \u0026quot;ipv4\u0026quot;\r}\r]\r}\rClean up debug deployment  exit kubectl delete -f controller-debug.yaml "
},
{
	"uri": "/extras/cni/",
	"title": "Amazon VPC CNI for Kubernetes",
	"tags": [],
	"description": "",
	"content": "How Amazon VPC CNI for k8s works As mentioned earlier, the Amazon VPC CNI for k8s is the addon in charge of assigning IP addresses to our Pods in an EKS cluster, by default it will assign real IP addresses from the Amazon VPC we have configured in our EKS cluster. Let\u0026rsquo;s discuss in more detail how it does work.\nConfig variables The Amazon VPC CNI for k8s allow multiple configuration variables that will modify its behaviour or even enable new features such as, prefix delegation, or custom networking. In this workshop we will focus only on the configuration variables that are most related to customer issues.\n WARM_ENI_TARGET WARM_IP_TARGET MINIMUM_IP_TARGET AWS_VPC_K8S_CNI_EXTERNALSNAT     Instance type WARM_ENI_TARGET WARM_IP_TARGET MINIMUM_IP_TARGET Pods Attached ENIs Attached Secondary IPs Unused IPs IPs per ENI     t3.small 1 - - 0 1 3 3 3   t3.small 1 - - 5 3 9 4 3,3,3   t3.small 1 - - 9 3 9 0 3,3,3              t3.small - 1 1 0 1 1 1 1   t3.small - 1 1 5 2 6 1 3,3   t3.small - 1 1 9 3 9 0 3,3,3              t3.small - 2 5 0 2 5 5 2,3   t3.small - 2 5 5 3 7 2 3,3,1   t3.small - 2 5 9 3 9 0 3,3,3              p3dn.24xlarge 1 - - 0 1 49 49 49   p3dn.24xlarge 1 - - 3 2 98 95 49,49   p3dn.24xlarge 1 - - 95 3 147 52 49,49,49              p3dn.24xlarge - 5 10 0 1 10 10 10   p3dn.24xlarge - 5 10 7 1 12 5 12   p3dn.24xlarge - 5 10 15 1 20 5 20   p3dn.24xlarge - 5 10 45 2 50 5 49,1                From https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md\n Calculate max IPs per Worker Node How many IP addresses can be assigned to a single Worker Node depends on the EC2 instance type we select for our cluster, in addition there are multiple combination of configuration we can set in the CNI that can modify the number of IP addresses allowed pero node, be aware that modifying the number of IP addresses to be assigned per node will directly impact the maximum number of pods that can be executed in the node.\nThe EKS team provide a simple bash script that you can use to calculate the actual number of pods/IP that can be assigned to a node:\n Download the script  curl -sSL -o /tmp/max-pods-calculator.sh https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/files/max-pods-calculator.sh cd /tmp Run the script using different parameters  bash ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.10.3\r29\rbash ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.10.3 --cni-custom-networking-enabled\r20\rbash ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.10.3 --cni-prefix-delegation-enabled\r110\rbash ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.10.3 --cni-prefix-delegation-enabled --show-max-allowed\r434\rHands On Lab  In this lab we will quickly see how modifying the CNI configuration will directly impact the number of available IP address in our VPC subnets. Remember, by default the VPC CNI is set with WARM_ENI_TARGET to \u0026ldquo;1\u0026rdquo;.    Open the VPC Subnet console and check current number of available IPv4 addresses:   Select on of the Private Subnets and check the Available IPv4 addresses value   Edit the aws-node daemonset to change the configuration\n  kubectl edit daemonsets.apps -n kube-system aws-node Look for the following configuration and remove it   - name: WARM_ENI_TARGET\rvalue: \u0026quot;1\u0026quot;\rAdd the following configuration, save and close   - name: WARM_IP_TARGET\rvalue: \u0026quot;1\u0026quot;\r- name: MINIMUM_IP_TARGET\rvalue: \u0026quot;1\u0026quot;\rWait a couple of minutes to get new aws_node pods deployed Go back to the VPC console and check again the Available IPv4 addresses value   Resources  Amazon VPC CNI for k8s Deep Dive ENI and IP Target Troubleshoot Amazon VPC CNI for k8s  "
},
{
	"uri": "/cleanup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Add instructions to clean up the environment\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Troubleshooting EKS Clusters",
	"tags": [],
	"description": "",
	"content": "During this workshop, the assistants will be presented with several failures that they will need to troubleshoot and fix. Topics covered in this workshop:\n Kubernetes components Kubernetes control and data plane logs and metrics Kubernetes Scheduler Scalability Kubernetes Networking  "
}]